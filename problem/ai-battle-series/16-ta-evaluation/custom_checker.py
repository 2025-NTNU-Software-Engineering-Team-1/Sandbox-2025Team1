#!/usr/bin/env python3
"""
AI-Powered Custom Checker for Problem 16: TA Evaluation

This checker validates the evaluation.json file generated by participants and
uses AI API to semantically evaluate the quality of the evaluations.

Evaluation Criteria:
- Specificity (30%): Whether comments are specific with examples
- Fairness (30%): Whether evaluations are objective and well-reasoned
- Constructiveness (25%): Whether improvement suggestions are provided
- Clarity (15%): Whether text is clear and organized
"""
import sys
import os
import json

DEBUG = os.environ.get("CHECKER_DEBUG", "1").strip().lower() not in (
    "",
    "0",
    "false",
    "no",
    "off",
)


def debug_log(message):
    if DEBUG:
        print(f"[DEBUG] {message}", file=sys.stderr)


def validate_json_structure(evaluation_data):
    """
    Validate the structure of evaluation.json.
    
    Returns:
        tuple: (is_valid, error_message)
    """
    required_ta_names = {"ChatGPT", "Gemini", "Opus"}

    # Check top-level keys
    if "evaluations" not in evaluation_data:
        return False, "Missing 'evaluations' key"

    if "overall_comment" not in evaluation_data:
        return False, "Missing 'overall_comment' key"

    if "would_recommend" not in evaluation_data:
        return False, "Missing 'would_recommend' key"

    evaluations = evaluation_data.get("evaluations", [])

    if not isinstance(evaluations, list) or len(evaluations) != 3:
        return False, "Expected exactly 3 evaluations (one for each TA)"

    found_tas = set()

    for i, eval_item in enumerate(evaluations):
        # Check required fields
        required_fields = [
            "ta_name", "score", "comment", "strengths", "improvements"
        ]
        for field in required_fields:
            if field not in eval_item:
                return False, f"Evaluation {i+1} missing '{field}' field"

        ta_name = eval_item.get("ta_name")
        if ta_name not in required_ta_names:
            return False, f"Unknown TA name: '{ta_name}'. Expected one of {required_ta_names}"

        if ta_name in found_tas:
            return False, f"Duplicate evaluation for TA: '{ta_name}'"
        found_tas.add(ta_name)

        # Validate score
        score = eval_item.get("score")
        if not isinstance(score, (int, float)) or score < 1 or score > 10:
            return False, f"Invalid score for {ta_name}: {score}. Expected 1-10"

        # Validate comment length
        comment = eval_item.get("comment", "")
        if len(comment) < 20:
            return False, f"Comment for {ta_name} is too short (minimum 20 characters)"

        # Validate arrays
        if not isinstance(eval_item.get("strengths"), list) or len(
                eval_item.get("strengths", [])) < 1:
            return False, f"No strengths listed for {ta_name}"

        if not isinstance(eval_item.get("improvements"), list) or len(
                eval_item.get("improvements", [])) < 1:
            return False, f"No improvements listed for {ta_name}"

    # Check if all TAs are evaluated
    if found_tas != required_ta_names:
        missing = required_ta_names - found_tas
        return False, f"Missing evaluation for: {missing}"

    # Validate overall_comment
    overall = evaluation_data.get("overall_comment", "")
    if len(overall) < 20:
        return False, "Overall comment is too short (minimum 20 characters)"

    # Validate would_recommend
    if not isinstance(evaluation_data.get("would_recommend"), bool):
        return False, "'would_recommend' must be a boolean (true/false)"

    return True, "Valid structure"


def check_quality_heuristics(evaluation_data):
    """
    Check evaluation quality using heuristics.
    
    Returns:
        tuple: (score 0-100, issues list)
    """
    issues = []
    score = 100

    evaluations = evaluation_data.get("evaluations", [])

    # Check for copy-paste evaluations
    comments = [e.get("comment", "") for e in evaluations]
    if len(set(comments)) < len(comments):
        issues.append("Duplicate comments detected")
        score -= 30

    # Check if all scores are the same (suspicious)
    scores = [e.get("score", 0) for e in evaluations]
    if len(set(scores)) == 1:
        if scores[0] <= 2 or scores[0] >= 9:
            issues.append(
                "All scores are the same extreme value without differentiation"
            )
            score -= 20

    # Check for empty or too short content
    for eval_item in evaluations:
        ta = eval_item.get("ta_name", "Unknown")

        # Check strengths quality
        strengths = eval_item.get("strengths", [])
        for s in strengths:
            if len(str(s)) < 3:
                issues.append(f"Very short strength for {ta}")
                score -= 5

        # Check improvements quality
        improvements = eval_item.get("improvements", [])
        for imp in improvements:
            if len(str(imp)) < 3:
                issues.append(f"Very short improvement for {ta}")
                score -= 5

    return max(0, score), issues


def evaluate_with_ai(api_key, model, evaluation_data):
    """
    Use Google Gemini API to semantically evaluate the quality of evaluations.
    
    Returns:
        tuple: (status, message, details)
    """
    try:
        import google.generativeai as genai

        genai.configure(api_key=api_key)
        ai_model = genai.GenerativeModel(model)

        prompt = f"""你是一個評鑑品質審核系統。請評估以下 AI 助教評鑑表的品質。

評鑑資料：
```json
{json.dumps(evaluation_data, ensure_ascii=False, indent=2)}
```

評估標準：
1. 具體性 (30%): 評語是否具體、有例子支撐
2. 公正性 (30%): 評價是否客觀、有理有據
3. 建設性 (25%): 是否提供可行的改進建議
4. 表達能力 (15%): 文字是否清晰、有條理

請根據以上標準評分（0-100分），並提供簡短說明。

回覆格式（請嚴格遵守）：
SCORE: [0-100]
VERDICT: [PASS 或 FAIL]
FEEDBACK: [一句話反饋]
"""

        debug_log(f"AI prompt length: {len(prompt)}")

        response = ai_model.generate_content(prompt)
        result_text = response.text.strip()

        debug_log(f"AI response: {result_text[:200]}")

        # Parse response
        lines = result_text.split('\n')
        score = 0
        verdict = "FAIL"
        feedback = ""

        for line in lines:
            line = line.strip()
            if line.startswith("SCORE:"):
                try:
                    score = int(line.split(":")[1].strip())
                except ValueError:
                    pass
            elif line.startswith("VERDICT:"):
                v = line.split(":")[1].strip().upper()
                if "PASS" in v:
                    verdict = "PASS"
            elif line.startswith("FEEDBACK:"):
                feedback = line.split(":", 1)[1].strip()

        if verdict == "PASS" and score >= 60:
            return "AC", f"AI Evaluation: PASS (Score: {score}/100)", feedback
        else:
            return "WA", f"AI Evaluation: FAIL (Score: {score}/100)", feedback

    except ImportError:
        debug_log("google-generativeai not available")
        return None, None, None
    except Exception as e:
        debug_log(f"AI evaluation error: {e}")
        return None, None, str(e)


def check(input_file, output_file, answer_file):
    """
    AI-Powered Custom Checker for TA Evaluation.
    
    This checker:
    1. Validates the stdout message
    2. Reads and validates evaluation.json from working directory
    3. Uses AI to evaluate the quality of the evaluations
    
    Args:
        input_file: Path to input file (not used for this problem)
        output_file: Path to student output file (stdout)
        answer_file: Path to expected answer file
    
    Returns:
        tuple: (status, message) where status is "AC" or "WA"
    """
    api_key = os.environ.get('AI_API_KEY')
    model = os.environ.get('AI_MODEL', 'gemini-2.5-flash')

    try:
        debug_log(
            f"files: input={input_file} output={output_file} answer={answer_file}"
        )
        debug_log(f"ai_config: api_key_present={bool(api_key)} model={model}")

        # Read stdout
        with open(output_file, 'r', encoding='utf-8') as f:
            stdout_content = f.read().strip()

        debug_log(f"stdout: {stdout_content[:100]}")

        # Check stdout message
        expected_stdout = "Evaluation submitted successfully!"
        if expected_stdout not in stdout_content:
            return "WA", f"Expected stdout to contain: '{expected_stdout}'"

        # Try to find evaluation.json in various locations
        working_dir = os.path.dirname(output_file)
        possible_paths = [
            os.path.join(working_dir, "evaluation.json"),
            "evaluation.json",
            os.path.join(os.getcwd(), "evaluation.json"),
        ]

        evaluation_json_path = None
        for path in possible_paths:
            if os.path.exists(path):
                evaluation_json_path = path
                break

        if not evaluation_json_path:
            return "WA", "evaluation.json not found. Please create this file."

        debug_log(f"Found evaluation.json at: {evaluation_json_path}")

        # Read and parse evaluation.json
        try:
            with open(evaluation_json_path, 'r', encoding='utf-8') as f:
                evaluation_data = json.load(f)
        except json.JSONDecodeError as e:
            return "WA", f"Invalid JSON in evaluation.json: {e}"

        debug_log(f"evaluation_data keys: {list(evaluation_data.keys())}")

        # Validate structure
        is_valid, error_msg = validate_json_structure(evaluation_data)
        if not is_valid:
            return "WA", f"Invalid structure: {error_msg}"

        debug_log("JSON structure validated successfully")

        # Check quality with heuristics
        heuristic_score, issues = check_quality_heuristics(evaluation_data)
        debug_log(f"Heuristic score: {heuristic_score}, issues: {issues}")

        if heuristic_score < 50:
            return "WA", f"Low quality evaluation: {', '.join(issues)}"

        # If AI API is available, use semantic evaluation
        if api_key:
            try:
                debug_log("Starting AI evaluation")
                status, message, feedback = evaluate_with_ai(
                    api_key, model, evaluation_data)
                if status:
                    debug_log(f"AI result: {status}, {message}")
                    return status, message
            except Exception as e:
                debug_log(f"AI evaluation failed: {e}")

        # Fallback: pass if structure is valid and heuristic score is good
        if heuristic_score >= 70:
            return "AC", f"Evaluation accepted (Score: {heuristic_score}/100, AI disabled)"
        else:
            return "WA", f"Evaluation needs improvement (Score: {heuristic_score}/100): {', '.join(issues[:2])}"

    except FileNotFoundError as e:
        debug_log(f"file_not_found: {e.filename}")
        return "WA", f"File not found: {e.filename}"
    except Exception as e:
        debug_log(f"checker_error: {str(e)}")
        return "WA", f"Checker error: {str(e)}"


if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("STATUS: WA")
        print("MESSAGE: Invalid checker arguments (expected 3 file paths)")
        sys.exit(1)

    input_file = sys.argv[1]
    output_file = sys.argv[2]
    answer_file = sys.argv[3]

    status, message = check(input_file, output_file, answer_file)

    print(f"STATUS: {status}")
    print(f"MESSAGE: {message}")
